# -*- coding: utf-8 -*-
"""h-m-implicit-als-model-0-014-recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SB7diEnCTQr1kNG__1yljSwvOhNy9MDh

# H&M - Implicit ALS model

## Implicit ALS base model for the competition [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations).


[Implicit](https://github.com/benfred/implicit/) is a library for recommender models. In theory, it supports GPU out-of-the-box, but I haven't tried it yet.

In this notebook we use ALS (Alternating Least Squares), but the library supports a lot of other models with not many changes.

ALS is one of the most used ML models for recommender systems. It's a matrix factorization method based on SVD (it's actually an approximated, numerical version of SVD). Basically, ALS factorizes the interaction matrix (user x items) into two smaller matrices, one for item embeddings and one for user embeddings. These new matrices are built in a manner such that the multiplication of a user and an item gives (approximately) it's interaction score. This build embeddings for items and for users that live in the same vector space, allowing the implementation of recommendations as simple cosine distances between users and items. This is, the 12 items we recommend for a given user are the 12 items with their embedding vectors closer to the user embedding vector.

There are a lot of online resources explaining it. For example, [here](https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1).


Be aware that there was a breaking API change in a recent release of implicit (11 days ago): https://github.com/benfred/implicit/releases/tag/v0.5.0 so some thing in the documentation are off if you use the version that comes installed in the Kaggle environments. Anyway, this competition doesn't forbid Internet usage, so upgrading the package to its latest version fixes all.


---

**I have reverted the kernet to version 14 with a score of `0.014` for now. The scores above `0.014` are using the `0.02` model by Heng Zheng as the fallback strategy for cold-start users. Therefore, a `0.018` score is actually bad. I did those version with the hope that they would work nicely together, obtaining `>0.02`. Since it was not the case, reporting a `0.018` score is not accurate at all.**

If I can obtain a score that surpasses the `0.02`, using ALS + Heng's baseline, I will roll back to those versions again.


# Please, _DO_ upvote if you find this kernel useful or interesting!

# Imports
"""

# FYI:
# This pip command takes a lot with GPU enabled (~15 min)
# It works though. And GPU accelerates the process *a lot*.
# I am developing with GPU turned off and submitting with GPU turned on
# !pip install --upgrade implicit

import os; os.environ['OPENBLAS_NUM_THREADS']='1'
import numpy as np
import pandas as pd
import implicit
from scipy.sparse import coo_matrix
from implicit.evaluation import mean_average_precision_at_k

"""# Load dataframes"""

# !git clone https://github.com/Kuro2003/data_train_ai_recommendation_systems.git
# df = pd.read_csv('data_train_ai_recommendation_systems/comments.csv')

# new dataset from SE4AI
# !wget https://raw.githubusercontent.com/VongoctriZ/SE4AI/master/AI/_transactions.csv
df = pd.read_csv('_transactions.csv')



print(df.info())

# df = df[['create_at', 'created_by.id', 'product_id', 'rating']]
# df.rename(columns={'create_at': 't_dat', 'created_by.id': 'user_id', 'product_id': 'item_id'}, inplace=True)

df.rename(columns={'product_id':'item_id'},inplace=True)

# Convert 't_dat' to datetime
df['t_dat'] = pd.to_datetime(df['t_dat'])

# Extract only the date part
df['t_dat'] = df['t_dat'].dt.date

# df.head()

def random_date(start, end, n):
    """Generate a list of random dates between start and end with possible duplicates."""
    date_range = pd.date_range(start, end)
    return np.random.choice(date_range, n)

# Sử dụng hàm này để sinh ngày tháng ngẫu nhiên
def assign_random_dates(df, start_date='2024-01-01', end_date='2024-12-31'):
    """Assign random dates within the year 2024 to the 't_dat' column of the DataFrame with possible duplicates."""
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)

    # Generate random dates with possible duplicates
    df['t_dat'] = random_date(start, end, df.shape[0])
    return df

df = assign_random_dates(df)

print(df['t_dat'].nunique())

df = df.drop_duplicates(['t_dat','user_id', 'item_id'])

print(df.head())

print(df.shape)

"""## Assign autoincrementing ids starting from 0 to both users and items

## Create coo_matrix (user x item) and csr matrix (user x item)

It is common to use scipy sparse matrices in recommender systems, because the main core of the problem is typically modeled as a matrix with users and items, with the values representing whether the user purchased (or liked) an items. Since each user purchases only a small fraction of the catalog of products, this matrix is full of zero (aka: it's sparse).

In a very recent release they did an API breaking change, so be aware of that: https://github.com/benfred/implicit/releases
In this notebook we are using the latest version, so everything is aligned with (user x item)

**We are using (user x item) matrices, both for training and for evaluating/recommender.**

In the previous versions the training procedure required a COO item x user

For evaluation and prediction, on the other hand, CSR matrices with users x items format should be provided.


### About COO matrices
COO matrices are a kind of sparse matrix.
They store their values as tuples of `(row, column, value)` (the coordinates)

You can read more about them here:
* https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)
* https://scipy-lectures.org/advanced/scipy_sparse/coo_matrix.html

From https://het.as.utexas.edu/HET/Software/Scipy/generated/scipy.sparse.coo_matrix.html

```python
>>> row  = np.array([0,3,1,0]) # user_ids
>>> col  = np.array([0,3,1,2]) # item_ids
>>> data = np.array([4,5,7,9]) # a bunch of ones of lenght unique(user) x unique(items)
>>> coo_matrix((data,(row,col)), shape=(4,4)).todense()
matrix([[4, 0, 9, 0],
        [0, 7, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 5]])
```

## About CSR matrices
* https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)
"""

# Create mappings for user_id and item_id to be zero-indexed
user_id_mapping = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())}
item_id_mapping = {item_id: idx for idx, item_id in enumerate(df['item_id'].unique())}

# Create mappings for user_id and item_id to be zero-indexed
user_id_mapping = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())}
item_id_mapping = {item_id: idx for idx, item_id in enumerate(df['item_id'].unique())}

# Apply the mappings to the dataframe
df['user_id'] = df['user_id'].map(user_id_mapping)
df['item_id'] = df['item_id'].map(item_id_mapping)

# Define the number of users and items
num_users = df['user_id'].nunique()
num_items = df['item_id'].nunique()

# Create the COO matrix
row = df['user_id'].values
col = df['item_id'].values
data = np.ones(df.shape[0])  # Use ones for implicit feedback

# Create the COO matrix with the shape based on unique users and items
coo_train = coo_matrix((data, (row, col)), shape=(num_users, num_items))

print("COO matrix shape:", coo_train.shape)
print("Non-zero elements:", coo_train.nnz)

"""# Check that model works ok with data"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = implicit.als.AlternatingLeastSquares(factors=10, iterations=2)
# model.fit(coo_train)

"""# Validation

## Functions required for validation
"""

def to_user_item_coo(df, user_id_map, item_id_map):
    """ Turn a dataframe with transactions into a COO sparse items x users matrix"""
    row = df['user_id'].map(user_id_map).values
    col = df['item_id'].map(item_id_map).values
    data = np.ones(df.shape[0])
    coo = coo_matrix((data, (row, col)), shape=(len(user_id_map), len(item_id_map)))
    return coo

def split_data(df, validation_days=7):
    """ Split a pandas dataframe into training and validation data, using <<validation_days>> """
    validation_cut = df['t_dat'].max() - pd.Timedelta(validation_days)
    df_train = df[df['t_dat'] < validation_cut]
    df_val = df[df['t_dat'] >= validation_cut]
    return df_train, df_val

def get_val_matrices(df, validation_days=7):
    """ Split into training and validation and create various matrices """
    df_train, df_val = split_data(df, validation_days=validation_days)

    user_id_map = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())}
    item_id_map = {item_id: idx for idx, item_id in enumerate(df['item_id'].unique())}

    coo_train = to_user_item_coo(df_train, user_id_map, item_id_map)
    coo_val = to_user_item_coo(df_val, user_id_map, item_id_map)

    csr_train = coo_train.tocsr()
    csr_val = coo_val.tocsr()

    return {'coo_train': coo_train,
            'csr_train': csr_train,
            'csr_val': csr_val}

# Use the existing validate function with the above modifications

def validate(matrices, factors=200, iterations=20, regularization=0.01, show_progress=True):
    """ Train an ALS model with <<factors>> (embeddings dimension) for <<iterations>> over matrices and validate with MAP@12 """
    coo_train, csr_train, csr_val = matrices['coo_train'], matrices['csr_train'], matrices['csr_val']

    model = implicit.als.AlternatingLeastSquares(factors=factors,
                                                 iterations=iterations,
                                                 regularization=regularization,
                                                 random_state=42)

#     # Debugging the input matrices
#     print("coo_train shape:", coo_train.shape, "non-zero elements:", coo_train.nnz)
#     print("csr_train shape:", csr_train.shape, "non-zero elements:", csr_train.nnz)
#     print("csr_val shape:", csr_val.shape, "non-zero elements:", csr_val.nnz)

    model.fit(csr_train, show_progress=show_progress)  # Use csr_train for fitting

    map12 = mean_average_precision_at_k(model, csr_train, csr_val, K=12, show_progress=show_progress, num_threads=4)

    print(f"Factors: {factors:>3} - Iterations: {iterations:>2} - Regularization: {regularization:4.3f} ==> MAP@12: {map12:6.5f}")
    return map12

matrices = get_val_matrices(df)

print(matrices)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# Main loop
best_map12 = 0
best_params = {}
for factors in [40, 50, 60, 100, 200, 500, 1000]:
    for iterations in [3, 12, 14, 15, 20]:
        for regularization in [0.01]:
            try:
                matrices = get_val_matrices(df)
                map12 = validate(matrices, factors, iterations, regularization, show_progress=False)
                if map12 > best_map12:
                    best_map12 = map12
                    best_params = {'factors': factors, 'iterations': iterations, 'regularization': regularization}
                    print(f"Best MAP@12 found. Updating: {best_params}")
            except IndexError as e:
                print(f"IndexError: {e} with factors={factors}, iterations={iterations}, regularization={regularization}")

# Output the best parameters found
print("===================")
print(f"Best parameters: {best_params}")
print(f"Best MAP@12: {best_map12}")

del matrices

"""# Training over the full dataset"""

user_id_map = {user_id: idx for idx, user_id in enumerate(df['user_id'].unique())}
item_id_map = {item_id: idx for idx, item_id in enumerate(df['item_id'].unique())}
coo_train = to_user_item_coo(df, user_id_map, item_id_map)
csr_train = coo_train.tocsr()

def train(coo_train, factors=200, iterations=15, regularization=0.01, show_progress=True):
    model = implicit.als.AlternatingLeastSquares(factors=factors,
                                                 iterations=iterations,
                                                 regularization=regularization,
                                                 random_state=42)
    model.fit(coo_train, show_progress=show_progress)
    return model

print(best_params)

model = train(coo_train, **best_params)

"""# Predictition"""

def predict(model, csr_train, user_id_map, item_id_map, submission_name="submissions.csv"):
    preds = []
    batch_size = 2000
    to_generate = np.arange(len(user_id_map))
    for startidx in range(0, len(to_generate), batch_size):
        batch = to_generate[startidx : startidx + batch_size]
        ids, scores = model.recommend(batch, csr_train[batch], N=12, filter_already_liked_items=False)
        for i, userid in enumerate(batch):
            customer_id = user_id_map[userid]
            user_items = ids[i]
            article_ids = [str(item_id_map[item_id]) for item_id in user_items]  # Convert item IDs to strings
            preds.append((customer_id, ' '.join(article_ids)))

    df_preds = pd.DataFrame(preds, columns=['user_id', 'item_predict'])
    df_preds.to_csv(submission_name, index=False)

    # display(df_preds.head())
    print(df_preds.shape)

    return df_preds

# Commented out IPython magic to ensure Python compatibility.
# %%time
df_preds = predict(model, csr_train, user_id_map, item_id_map)

# Create inverse mappings
inverse_user_id_mapping = {idx: user_id for user_id, idx in user_id_mapping.items()}
inverse_item_id_mapping = {idx: item_id for item_id, idx in item_id_mapping.items()}

# Function to convert back user_id and item_id in the DataFrame
def convert_back(df, inverse_user_id_mapping, inverse_item_id_mapping):
    df['user_id'] = df['user_id'].map(inverse_user_id_mapping)
    df['item_predict'] = df['item_predict'].apply(
        lambda x: ' '.join(str(inverse_item_id_mapping[int(item)]) for item in x.split())
    )
    return df

# Convert back to original values
df_converted_back = convert_back(df_preds, inverse_user_id_mapping, inverse_item_id_mapping)

# Display the converted DataFrame
print(df_converted_back)

df_converted_back.to_csv('recommendations.csv')


# save the models and mappings

import joblib

# Save the model and mappings
joblib.dump(model, 'als_model.pkl')
joblib.dump(user_id_mapping, 'user_id_mapping.pkl')
joblib.dump(item_id_mapping, 'item_id_mapping.pkl')
joblib.dump(inverse_user_id_mapping, 'inverse_user_id_mapping.pkl')
joblib.dump(inverse_item_id_mapping, 'inverse_item_id_mapping.pkl')
